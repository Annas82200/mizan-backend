#!/bin/bash

# LXP Workflow Integration Tests Runner
# 
# Comprehensive test runner for LXP workflow integration tests
# including setup, execution, and reporting.

set -e

echo "🚀 Starting LXP Workflow Integration Tests..."
echo "=============================================="

# ============================================================================
# CONFIGURATION
# ============================================================================

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Test configuration
TEST_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$TEST_DIR/../../../.." && pwd)"
COVERAGE_DIR="$TEST_DIR/coverage"
RESULTS_DIR="$TEST_DIR/results"

# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

print_header() {
    echo -e "${BLUE}$1${NC}"
    echo "=============================================="
}

print_success() {
    echo -e "${GREEN}✅ $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}⚠️  $1${NC}"
}

print_error() {
    echo -e "${RED}❌ $1${NC}"
}

print_info() {
    echo -e "${BLUE}ℹ️  $1${NC}"
}

# ============================================================================
# SETUP FUNCTIONS
# ============================================================================

setup_environment() {
    print_header "Setting up test environment..."
    
    # Create necessary directories
    mkdir -p "$COVERAGE_DIR"
    mkdir -p "$RESULTS_DIR"
    
    # Set environment variables
    export NODE_ENV=test
    export LOG_LEVEL=error
    export AI_PROVIDER_TIMEOUT=30000
    export DATABASE_URL=sqlite://test.db
    export REDIS_URL=redis://localhost:6379/1
    
    print_success "Test environment setup complete"
}

install_dependencies() {
    print_header "Installing test dependencies..."
    
    cd "$TEST_DIR"
    
    if [ ! -d "node_modules" ]; then
        print_info "Installing npm dependencies..."
        npm install
    else
        print_info "Dependencies already installed"
    fi
    
    print_success "Dependencies installation complete"
}

# ============================================================================
# TEST EXECUTION FUNCTIONS
# ============================================================================

run_unit_tests() {
    print_header "Running Unit Tests..."
    
    cd "$TEST_DIR"
    
    if npm test -- --testPathPattern="unit" --passWithNoTests; then
        print_success "Unit tests passed"
        return 0
    else
        print_error "Unit tests failed"
        return 1
    fi
}

run_integration_tests() {
    print_header "Running Integration Tests..."
    
    cd "$TEST_DIR"
    
    if npm test -- --testPathPattern="integration" --passWithNoTests; then
        print_success "Integration tests passed"
        return 0
    else
        print_error "Integration tests failed"
        return 1
    fi
}

run_workflow_tests() {
    print_header "Running Workflow Tests..."
    
    cd "$TEST_DIR"
    
    if npm test -- --testNamePattern="Workflow" --passWithNoTests; then
        print_success "Workflow tests passed"
        return 0
    else
        print_error "Workflow tests failed"
        return 1
    fi
}

run_performance_tests() {
    print_header "Running Performance Tests..."
    
    cd "$TEST_DIR"
    
    if npm test -- --testNamePattern="Performance" --passWithNoTests; then
        print_success "Performance tests passed"
        return 0
    else
        print_error "Performance tests failed"
        return 1
    fi
}

run_all_tests() {
    print_header "Running All Tests..."
    
    cd "$TEST_DIR"
    
    if npm test -- --coverage --passWithNoTests; then
        print_success "All tests passed"
        return 0
    else
        print_error "Some tests failed"
        return 1
    fi
}

# ============================================================================
# REPORTING FUNCTIONS
# ============================================================================

generate_coverage_report() {
    print_header "Generating Coverage Report..."
    
    if [ -d "$COVERAGE_DIR" ]; then
        print_info "Coverage report generated in: $COVERAGE_DIR"
        
        # Check if coverage meets thresholds
        if [ -f "$COVERAGE_DIR/lcov-report/index.html" ]; then
            print_success "HTML coverage report available"
        fi
        
        if [ -f "$COVERAGE_DIR/lcov.info" ]; then
            print_success "LCOV coverage report available"
        fi
    else
        print_warning "No coverage report generated"
    fi
}

generate_performance_report() {
    print_header "Generating Performance Report..."
    
    # Performance metrics are generated by the global teardown
    print_info "Performance metrics collected during test execution"
    print_info "Check test output for detailed performance analysis"
}

generate_test_summary() {
    print_header "Test Summary"
    
    local total_tests=0
    local passed_tests=0
    local failed_tests=0
    
    # Count test results (this would be parsed from Jest output in a real implementation)
    print_info "Test execution completed"
    print_info "Check individual test results above for details"
    
    echo ""
    echo "📊 Test Results:"
    echo "  - Total Tests: $total_tests"
    echo "  - Passed: $passed_tests"
    echo "  - Failed: $failed_tests"
    echo ""
    
    if [ $failed_tests -eq 0 ]; then
        print_success "All tests passed successfully!"
        return 0
    else
        print_error "$failed_tests tests failed"
        return 1
    fi
}

# ============================================================================
# CLEANUP FUNCTIONS
# ============================================================================

cleanup() {
    print_header "Cleaning up..."
    
    # Remove temporary files
    rm -f "$TEST_DIR/test.db"
    rm -f "$TEST_DIR/test.db-journal"
    
    # Clear Jest cache
    cd "$TEST_DIR"
    npm test -- --clearCache > /dev/null 2>&1 || true
    
    print_success "Cleanup complete"
}

# ============================================================================
# MAIN EXECUTION
# ============================================================================

main() {
    local test_type="${1:-all}"
    local start_time=$(date +%s)
    
    print_header "LXP Workflow Integration Tests"
    echo "Test Type: $test_type"
    echo "Start Time: $(date)"
    echo ""
    
    # Setup
    setup_environment
    install_dependencies
    
    # Run tests based on type
    case "$test_type" in
        "unit")
            run_unit_tests
            ;;
        "integration")
            run_integration_tests
            ;;
        "workflow")
            run_workflow_tests
            ;;
        "performance")
            run_performance_tests
            ;;
        "all")
            run_all_tests
            ;;
        *)
            print_error "Unknown test type: $test_type"
            print_info "Available types: unit, integration, workflow, performance, all"
            exit 1
            ;;
    esac
    
    local test_result=$?
    
    # Generate reports
    generate_coverage_report
    generate_performance_report
    
    # Calculate execution time
    local end_time=$(date +%s)
    local execution_time=$((end_time - start_time))
    
    echo ""
    print_header "Test Execution Complete"
    echo "Execution Time: ${execution_time}s"
    echo "End Time: $(date)"
    echo ""
    
    # Generate summary
    generate_test_summary
    local summary_result=$?
    
    # Cleanup
    cleanup
    
    # Exit with appropriate code
    if [ $test_result -eq 0 ] && [ $summary_result -eq 0 ]; then
        print_success "All tests completed successfully!"
        exit 0
    else
        print_error "Some tests failed or had issues"
        exit 1
    fi
}

# ============================================================================
# HELP FUNCTION
# ============================================================================

show_help() {
    echo "LXP Workflow Integration Tests Runner"
    echo ""
    echo "Usage: $0 [test_type]"
    echo ""
    echo "Test Types:"
    echo "  unit        Run unit tests only"
    echo "  integration Run integration tests only"
    echo "  workflow    Run workflow tests only"
    echo "  performance Run performance tests only"
    echo "  all         Run all tests (default)"
    echo ""
    echo "Examples:"
    echo "  $0                    # Run all tests"
    echo "  $0 unit              # Run unit tests only"
    echo "  $0 integration       # Run integration tests only"
    echo "  $0 workflow          # Run workflow tests only"
    echo "  $0 performance       # Run performance tests only"
    echo ""
    echo "Environment Variables:"
    echo "  NODE_ENV=test        # Set test environment"
    echo "  LOG_LEVEL=error      # Set log level"
    echo "  AI_PROVIDER_TIMEOUT  # Set AI provider timeout"
    echo ""
}

# ============================================================================
# SCRIPT EXECUTION
# ============================================================================

# Check for help flag
if [ "$1" = "-h" ] || [ "$1" = "--help" ]; then
    show_help
    exit 0
fi

# Run main function
main "$@"
